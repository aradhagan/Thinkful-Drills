{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\nagad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Intro to word2vec\n",
    "\n",
    "The most common unsupervised neural network approach for NLP is word2vec, a shallow neural network model for converting words to vectors using distributed representation: Each word is represented by many neurons, and each neuron is involved in representing many words.  At the highest level of abstraction, word2vec assigns a vector of random values to each word.  For a word W, it looks at the words that are near W in the sentence, and shifts the values in the word vectors such that the vectors for words near that W are closer to the W vector, and vectors for words not near W are farther away from the W vector.  With a large enough corpus, this will eventually result in words that often appear together having vectors that are near one another, and words that rarely or never appear together having vectors that are far away from each other.  Then, using the vectors, similarity scores can be computed for each pair of words by taking the cosine of the vectors.  \n",
    "\n",
    "This may sound quite similar to the Latent Semantic Analysis approach you just learned.  The conceptual difference is that LSA creates vector representations of sentences based on the words in them, while word2vec creates representations of individual words, based on the words around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it good for?\n",
    "\n",
    "Word2vec is useful for any time when computers need to parse requests written by humans. The problem with human communication is that there are so many different ways to communicate the same concept. It's easy for us, as humans, to know that \"the silverware\" and \"the utensils\" can refer to the same thing. Computers can't do that unless we teach them, and this can be a real chokepoint for human/computer interactions. If you've ever played a text adventure game (think _Colossal Cave Adventure_ or _Zork_), you may have encountered the following scenario:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GAME: You are on a forest path north of the field. A cave leads into a granite butte to the north.\n",
    "A thick hedge blocks the way to the west.\n",
    "A hefty stick lies on the ground.\n",
    "\n",
    "YOU: pick up stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: lift stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: take stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: grab stick  \n",
    "\n",
    "GAME: You grab the stick from the ground and put it in your bag.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And your brain explodes from frustration. A text adventure game that incorporates a properly trained word2vec model would have vectors for \"pick up\", \"lift\", and \"take\" that are close to the vector for \"grab\" and therefore could accept those other verbs as synonyms so you could move ahead faster. In more practical applications, word2vec and other similar algorithms are what help a search engine return the best results for your query and not just the ones that contain the exact words you used. In fact, search is a better example, because not only does the search engine need to understand your request, it also needs to match it to web pages that were _also written by humans_ and therefore _also use idiosyncratic language_.\n",
    "\n",
    "Humans, man.  \n",
    "\n",
    "So how does it work?\n",
    "\n",
    "## Generating vectors: Multiple algorithms\n",
    "\n",
    "In considering the relationship between a word and its surrounding words, word2vec has two options that are the inverse of one another:\n",
    "\n",
    " * _Continuous Bag of Words_ (CBOW): the identity of a word is predicted using the words near it in a sentence.\n",
    " * _Skip-gram_: The identities of words are predicted from the word they surround. Skip-gram seems to work better for larger corpuses.\n",
    "\n",
    "For the sentence \"Terry Gilliam is a better comedian than a director\", if we focus on the word \"comedian\" then CBOW will try to predict \"comedian\" using \"is\", \"a\", \"better\", \"than\", \"a\", and \"director\".  Skip-gram will try to predict \"is\", \"a\", \"better\", \"than\", \"a\", and \"director\" using the word \"comedian\". In practice, for CBOW the vector for \"comedian\" will be pulled closer to the other words, while for skip-gram the vectors for the other words will be pulled closer to \"comedian\".  \n",
    "\n",
    "In addition to moving the vectors for nearby words closer together, each time a word is processed some vectors are moved farther away. Word2vec has two approaches to \"pushing\" vectors apart:\n",
    " \n",
    " * _Negative sampling_: Like it says on the tin, each time a word is pulled toward some neighbors, the vectors for a randomly chosen small set of other words are pushed away.\n",
    " * _Hierarchical softmax_: Every neighboring word is pulled closer or farther from a subset of words chosen based on a tree of probabilities.\n",
    "\n",
    "## What is similarity? Word2vec strengths and weaknesses\n",
    "\n",
    "Keep in mind that word2vec operates on the assumption that frequent proximity indicates similarity, but words can be \"similar\" in various ways. They may be conceptually similar (\"royal\", \"king\", and \"throne\"), but they may also be functionally similar (\"tremendous\" and \"negligible\" are both common modifiers of \"size\"). Here is a more detailed exploration, [with examples](https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/), of what \"similarity\" means in word2vec.\n",
    "\n",
    "One cool thing about word2vec is that it can identify similarities between words _that never occur near one another in the corpus_. For example, consider these sentences:\n",
    "\n",
    "\"The dog played with an elastic ball.\"\n",
    "\"Babies prefer the ball that is bouncy.\"\n",
    "\"I wanted to find a ball that's elastic.\"\n",
    "\"Tracy threw a bouncy ball.\"\n",
    "\n",
    "\"Elastic\" and \"bouncy\" are similar in meaning in the text but don't appear in the same sentence. However, both appear near \"ball\". In the process of nudging the vectors around so that \"elastic\" and \"bouncy\" are both near the vector for \"ball\", the words also become nearer to one another and their similarity can be detected.\n",
    "\n",
    "For a while after it was introduced, [no one was really sure why word2vec worked as well as it did](https://arxiv.org/pdf/1402.3722v1.pdf) (see last paragraph of the linked paper). A few years later, some additional math was developed to explain word2vec and similar models. If you are comfortable with both math and \"academese\", have a lot of time on your hands, and want to take a deep dive into the inner workings of word2vec, [check out this paper](https://arxiv.org/pdf/1502.03520v7.pdf) from 2016.  \n",
    "\n",
    "One of the draws of word2vec when it first came out was that the vectors could be used to convert analogies (\"king\" is to \"queen\" as \"man\" is to \"woman\", for example) into mathematical expressions (\"king\" + \"woman\" - \"man\" = ?) and solve for the missing element (\"queen\"). This is kinda nifty.\n",
    "\n",
    "A drawback of word2vec is that it works best with a corpus that is at least several billion words long. Even though the word2vec algorithm is speedy, this is a a lot of data and takes a long time! Our example dataset is only two million words long, which allows us to run it in the notebook without overwhelming the kernel, but probably won't give great results.  Still, let's try it!\n",
    "\n",
    "There are a few word2vec implementations in Python, but the general consensus is the easiest one to us is in [gensim](https://radimrehurek.com/gensim/models/word2vec.html). Now is a good time to `pip install gensim` if you don't have it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text[0:900000]\n",
    "\n",
    "\n",
    "# Import all the Austen in the Project Gutenberg corpus.\n",
    "austen = \"\"\n",
    "for novel in ['persuasion','emma','sense']:\n",
    "    work = gutenberg.raw('austen-' + novel + '.txt')\n",
    "    austen = austen + work\n",
    "\n",
    "# Clean the data.\n",
    "austen_clean = text_cleaner(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\nagad\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "symbolic link created for C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Linking successful\n",
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# Parse the data. This can take some time.\n",
    "#!python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "austen_doc = nlp(austen_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lady', 'russell', 'steady', 'age', 'character', 'extremely', 'provide', 'thought', 'second', 'marriage', 'need', 'apology', 'public', 'apt', 'unreasonably', 'discontent', 'woman', 'marry', 'sir', 'walter', 'continue', 'singleness', 'require', 'explanation']\n",
      "We have 9298 sentences and 900000 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Organize the parsed doc into sentences, while filtering out punctuation\n",
    "# and stop words, and converting words to lower case lemmas.\n",
    "sentences = []\n",
    "for sentence in austen_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "\n",
    "print(sentences[20])\n",
    "print('We have {} sentences and {} tokens.'.format(len(sentences), len(austen_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('benwick', 0.9454872012138367), ('goddard', 0.9453417062759399), ('musgrove', 0.9420642852783203), ('harville', 0.9413917660713196), ('charles', 0.9165124297142029), ('clay', 0.9125531315803528), ('croft', 0.9056294560432434), ('wentworth', 0.9022759199142456), ('colonel', 0.854128360748291), ('room', 0.8507972955703735)]\n",
      "+++\n",
      "0.92158103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sister\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "print ('+++')\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"car train bus sister\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Clearly this model is not great – while some words given above might possibly fill in the analogy woman:lady::man:?, most answers likely make little sense. You'll notice as well that re-running the model likely gives you different results, indicating random chance plays a large role here.\n",
    "\n",
    "We do, however, get a nice result on \"marriage\" being dissimilar to \"breakfast\", \"lunch\", and \"dinner\". \n",
    "\n",
    "## Drill 0\n",
    "\n",
    "Take a few minutes to modify the hyperparameters of this model and see how its answers change. Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def model_fn(model):\n",
    "    \n",
    "    model=model\n",
    "    print('done!')\n",
    "    # List of words in model.\n",
    "    vocab = model.wv.vocab.keys()\n",
    "\n",
    "    print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "    \n",
    "    # Similarity is calculated using the cosine, so again 1 is total\n",
    "    # similarity and 0 is no similarity.\n",
    "    print('\\nThe similarity score of mr. and mrs. ', model.wv.similarity('mr', 'mrs'))\n",
    "    print('The similarity score of sister and brother ', model.wv.similarity('sister', 'brother'))\n",
    "    print('The similarity score of sofa and forget ', model.wv.similarity('sofa', 'forget'))\n",
    "    print('The similarity score of breakfast and marriage ', model.wv.similarity('breakfast', 'marriage'))\n",
    "    print('The similarity score of preserve and hall ', model.wv.similarity('preserve', 'hall'))\n",
    "    # One of these things is not like the other...\n",
    "       \n",
    "    print ('\\nBelow are the words similar to \"husband\"')\n",
    "    [ print(item) for item in model.wv.most_similar(['husband'])]\n",
    "    print ('\\nBelow are the words similar to \"Wife\"')\n",
    "    [ print(item) for item in model.wv.most_similar(['wife'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "[('solicitude', 0.7335948944091797), ('event', 0.6837687492370605), ('removal', 0.6537907123565674), ('remove', 0.6537649035453796), ('change', 0.6516532897949219), ('endeavour', 0.6466081142425537), ('advantage', 0.6412881016731262), ('plan', 0.6308327913284302), ('compare', 0.6262389421463013), ('independence', 0.6251139640808105)]\n",
      "\n",
      "The similarity score of mr. and mrs.  0.71513325\n",
      "The similarity score of sister and brother  0.8267086\n",
      "The similarity score of sofa and forget  0.6389605\n",
      "The similarity score of breakfast and marriage  0.10403478\n",
      "The similarity score of preserve and hall  0.4787202\n",
      "\n",
      "Below are the words similar to \"husband\"\n",
      "('wife', 0.9193407893180847)\n",
      "('unhappy', 0.8721081018447876)\n",
      "('especially', 0.8526115417480469)\n",
      "('luck', 0.8504742383956909)\n",
      "('want', 0.8488104343414307)\n",
      "('thousand', 0.8440971374511719)\n",
      "('oppose', 0.8435943722724915)\n",
      "('easy', 0.8431147336959839)\n",
      "('odd', 0.8384135365486145)\n",
      "('anybody', 0.8360109329223633)\n",
      "\n",
      "Below are the words similar to \"Wife\"\n",
      "('husband', 0.9193407893180847)\n",
      "('easy', 0.8445366621017456)\n",
      "('match', 0.8361519575119019)\n",
      "('good', 0.8359988927841187)\n",
      "('oppose', 0.8309340476989746)\n",
      "('especially', 0.8280379772186279)\n",
      "('observation', 0.8239565491676331)\n",
      "('judge', 0.8216612935066223)\n",
      "('luck', 0.8212140798568726)\n",
      "('unhappy', 0.8197883367538452)\n"
     ]
    }
   ],
   "source": [
    "# Tinker with hyperparameters here.\n",
    "model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=10,  # Minimum word count threshold.\n",
    "        window=20,      # Number of words around target word to consider.\n",
    "        sg=1,          # Use CBOW because our corpus is small.\n",
    "        sample=1e-3 ,  # Penalize frequent words.\n",
    "        size=300,      # Word vector length.\n",
    "        hs=1,           # Use hierarchical softmax.\n",
    "\n",
    ")\n",
    "model_fn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "[('benwick', 0.9540441036224365), ('shirley', 0.9408626556396484), ('mary', 0.9350240230560303), ('louisa', 0.9296693205833435), ('henrietta', 0.9270414710044861), ('navy', 0.9266721606254578), ('prefer', 0.9250074625015259), ('harville', 0.916487991809845), ('musgrove', 0.9151268601417542), ('sufficient', 0.9132134914398193)]\n",
      "\n",
      "The similarity score of mr. and mrs.  0.973672\n",
      "The similarity score of sister and brother  0.95952964\n",
      "The similarity score of sofa and forget  0.9532353\n",
      "The similarity score of breakfast and marriage  0.98132324\n",
      "The similarity score of preserve and hall  0.9061974\n",
      "\n",
      "Below are the words similar to \"husband\"\n",
      "('chuse', 0.9985018968582153)\n",
      "('complaint', 0.9980168342590332)\n",
      "('sort', 0.9979889988899231)\n",
      "('hear', 0.9979228377342224)\n",
      "('voice', 0.997841477394104)\n",
      "('relief', 0.9977630376815796)\n",
      "('fit', 0.9972444176673889)\n",
      "('ah', 0.9971776008605957)\n",
      "('reserve', 0.9971638917922974)\n",
      "('small', 0.9970738887786865)\n",
      "\n",
      "Below are the words similar to \"Wife\"\n",
      "('danger', 0.9988630414009094)\n",
      "('world', 0.9987934827804565)\n",
      "('reason', 0.998742938041687)\n",
      "('warmly', 0.9987292289733887)\n",
      "('safely', 0.9986265897750854)\n",
      "('hard', 0.9985848665237427)\n",
      "('like', 0.9985787868499756)\n",
      "('hearted', 0.9985494613647461)\n",
      "('unpleasant', 0.9984915256500244)\n",
      "('independence', 0.9983513951301575)\n"
     ]
    }
   ],
   "source": [
    "# Tinker with hyperparameters here.\n",
    "model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=10,  # Minimum word count threshold.\n",
    "        window=20,      # Number of words around target word to consider.\n",
    "        sg=0,          # Use CBOW because our corpus is small.\n",
    "        sample=1e-3 ,  # Penalize frequent words.\n",
    "        size=300,      # Word vector length.\n",
    "        hs=1,           # Use hierarchical softmax.\n",
    "\n",
    ")\n",
    "model_fn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "[('question', 0.6951432228088379), ('meet', 0.6486757397651672), ('ask', 0.6413214206695557), ('report', 0.6333310604095459), ('removal', 0.623195469379425), ('christmas', 0.619205117225647), ('set', 0.6134544014930725), ('tired', 0.6099216938018799), ('business', 0.607487142086029), ('crofts', 0.6070114374160767)]\n",
      "\n",
      "The similarity score of mr. and mrs.  0.72404826\n",
      "The similarity score of sister and brother  0.8388392\n",
      "The similarity score of sofa and forget  0.68120646\n",
      "The similarity score of breakfast and marriage  0.08157115\n",
      "The similarity score of preserve and hall  0.5351209\n",
      "\n",
      "Below are the words similar to \"husband\"\n",
      "('wife', 0.9165480136871338)\n",
      "('unhappy', 0.8616642355918884)\n",
      "('generally', 0.8479317426681519)\n",
      "('fellow', 0.84563148021698)\n",
      "('humour', 0.8435673713684082)\n",
      "('preserve', 0.842993438243866)\n",
      "('luck', 0.8324763774871826)\n",
      "('inform', 0.8306639790534973)\n",
      "('consider', 0.8305859565734863)\n",
      "('capable', 0.8303912878036499)\n",
      "\n",
      "Below are the words similar to \"Wife\"\n",
      "('husband', 0.916547954082489)\n",
      "('humour', 0.8678648471832275)\n",
      "('capable', 0.8431548476219177)\n",
      "('support', 0.8304425477981567)\n",
      "('unhappy', 0.8200856447219849)\n",
      "('preserve', 0.8199576139450073)\n",
      "('match', 0.8176605105400085)\n",
      "('fair', 0.8167376518249512)\n",
      "('consider', 0.7957160472869873)\n",
      "('luck', 0.7949212789535522)\n"
     ]
    }
   ],
   "source": [
    "# Tinker with hyperparameters here.\n",
    "model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "        min_count=10,  # Minimum word count threshold.\n",
    "        window=25,      # Number of words around target word to consider.\n",
    "        sg=1,          # Use CBOW because our corpus is small.\n",
    "        sample=1e-3 ,  # Penalize frequent words.\n",
    "        size=300,      # Word vector length.\n",
    "        hs=1,           # Use hierarchical softmax.\n",
    "\n",
    ")\n",
    "model_fn(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chaning window parameter to 20 the similarity score increased to 84 i.e. it gave mr and mrs as  84% similar now compared to 75% similar.\n",
    "\n",
    "By changing the sample penalty i.e made to 1e-4 the similarity of mr. and mrs. went to 99% similar.\n",
    "\n",
    "But this needs to be tested with other words since mr and mrs is not very similar too, there is a huge differnce of gender.\n",
    "\n",
    "as shown above...it is giving the high similarity scores of 89% to 99% for even two unrelated words like sofa and forget and breakfast and marriage.\n",
    "\n",
    "Tested all other parameters with different values and found that it worsens the similarity scores..ie. it gives sofa and forget as the highly similar and breakfast and marriage and preserve and hall as highly similar..which we not isnt. so it is better off not to change these values.\n",
    "\n",
    "the other parameter which can be tweaked is sg which says to use CBOW features with value 1. But changing it to value 0 i.e not to use CBOW features made the scores worst. i.e distantly related words showed similarity.\n",
    "\n",
    "the best parameter settings are the described in the last cell above. with window=25. this canbe categorized as best here becuase the unrelated words like sofa and forget, breakfast and marriage etc. are showing minimum scores and mr and mrs and sister and brother are showing higher similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even the words related to husband and wife make more sense with the last final hyper parameter change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Example word2vec applications\n",
    "\n",
    "You can use the vectors from word2vec as features in other models, or try to gain insight from the vector compositions themselves.\n",
    "\n",
    "Here are some neat things people have done with word2vec:\n",
    "\n",
    " * [Visualizing word embeddings in Jane Austen's Pride and Prejudice](http://blogger.ghostweather.com/2014/11/visualizing-word-embeddings-in-pride.html). Skip to the bottom to see a _truly honest_ account of this data scientist's process.\n",
    "\n",
    " * [Tracking changes in Dutch Newspapers' associations with words like 'propaganda' and 'alien' from 1950 to 1990](https://www.slideshare.net/MelvinWevers/concepts-through-time-tracing-concepts-in-dutch-newspaper-discourse-using-sequential-word-vector-spaces).\n",
    "\n",
    " * [Helping customers find clothing items similar to a given item but differing on one or more characteristics](http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1: Word2Vec on 100B+ words\n",
    "\n",
    "As we mentioned, word2vec really works best on a big corpus, but it can take half a day to clean such a corpus and run word2vec on it.  Fortunately, there are word2vec models available that have already been trained on _really_ big corpora. They are big files, but you can download a [pretrained model of your choice here](https://github.com/3Top/word2vec-api). At minimum, the ones built with word2vec (check the \"Architecture\" column) should load smoothly using an appropriately modified version of the code below, and you can play to your heart's content.\n",
    "\n",
    "Because the models are so large, however, you may run into memory problems or crash the kernel. If you can't get a pretrained model to run locally, check out this [interactive web app of the Google News model](https://rare-technologies.com/word2vec-tutorial/#bonus_app) instead.\n",
    "\n",
    "However you access it, play around with a pretrained model. Is there anything interesting you're able to pull out about analogies, similar words, or words that don't match? Write up a quick note about your tinkering and discuss it with your mentor during your next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format ('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagad\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.9219692945480347),\n",
       " ('mother', 0.8470973968505859),\n",
       " ('daughter', 0.8451575636863708),\n",
       " ('father', 0.8430609107017517),\n",
       " ('friend', 0.8267569541931152),\n",
       " ('son', 0.7971027493476868),\n",
       " ('brother', 0.7917457222938538),\n",
       " ('married', 0.7847107648849487),\n",
       " ('girlfriend', 0.7784738540649414),\n",
       " ('boyfriend', 0.7674957513809204)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play around with your pretrained model here.\n",
    "word_vectors.most_similar(['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughter', 0.9242234230041504),\n",
       " ('husband', 0.9219692945480347),\n",
       " ('mother', 0.902587890625),\n",
       " ('father', 0.8440753817558289),\n",
       " ('sister', 0.8372182846069336),\n",
       " ('friend', 0.8299500942230225),\n",
       " ('married', 0.8200123310089111),\n",
       " ('niece', 0.8161543011665344),\n",
       " ('widow', 0.810448169708252),\n",
       " ('son', 0.8092067837715149)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(['wife'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe here that the same words husband and wife give totally different results with different models trained on different datasets albeit they are both word2vec models.\n",
    "\n",
    "So what we learn from this is that based on what data the word2vec learns the word associations it spits out the similar associations it has seen when asked.\n",
    "Since the google database it was trained is a much bigger associations corpus it has all the relationships generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534157752991),\n",
       " ('prince', 0.6517034769058228),\n",
       " ('elizabeth', 0.6464517712593079),\n",
       " ('mother', 0.631171703338623),\n",
       " ('emperor', 0.6106470227241516),\n",
       " ('wife', 0.6098655462265015)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is giving all the words similar to the queen answer...which is ranked the first basedon the vector similarity score measured by the cosine of the angle between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.7514394521713257),\n",
       " ('daughter', 0.6816167831420898),\n",
       " ('queen', 0.6580643653869629),\n",
       " ('duchess', 0.6560284495353699),\n",
       " ('niece', 0.6351829767227173),\n",
       " ('wife', 0.632085919380188),\n",
       " ('eldest', 0.6206661462783813),\n",
       " ('married', 0.6112802028656006),\n",
       " ('cousin', 0.611089825630188),\n",
       " ('throne', 0.6054269075393677)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'prince'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was expecting a princess there it did that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8323494"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19185662"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('forget', 'sofa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.960464477539063e-08"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.distance(\"media\", \"media\") ## it shows practically as zero i.e identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1201925277709961"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.distance(\"cat\", \"dog\")## they are also close in distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0028903558850288"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.distance(\"king\", \"stool\")## they are very distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6529471278190613"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.distance(\"king\", \"rich\")## A king is more closer to the rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091540098190308"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.distance(\"king\", \"poor\")## and a king is more distant to the poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24934  ,  0.68318  , -0.044711 , -1.3842   , -0.0073079,\n",
       "        0.651    , -0.33958  , -0.19785  , -0.33925  ,  0.26691  ,\n",
       "       -0.033062 ,  0.15915  ,  0.89547  ,  0.53999  , -0.55817  ,\n",
       "        0.46245  ,  0.36722  ,  0.1889   ,  0.83189  ,  0.81421  ,\n",
       "       -0.11835  , -0.53463  ,  0.24158  , -0.038864 ,  1.1907   ,\n",
       "        0.79353  , -0.12308  ,  0.6642   , -0.77619  , -0.45713  ,\n",
       "       -1.054    , -0.20557  , -0.13296  ,  0.12239  ,  0.88458  ,\n",
       "        1.024    ,  0.32288  ,  0.82105  , -0.069367 ,  0.024211 ,\n",
       "       -0.51418  ,  0.8727   ,  0.25759  ,  0.91526  , -0.64221  ,\n",
       "        0.041159 , -0.60208  ,  0.54631  ,  0.66076  ,  0.19796  ,\n",
       "       -1.1393   ,  0.79514  ,  0.45966  , -0.18463  , -0.64131  ,\n",
       "       -0.24929  , -0.40194  , -0.50786  ,  0.80579  ,  0.53365  ,\n",
       "        0.52732  ,  0.39247  , -0.29884  ,  0.009585 ,  0.99953  ,\n",
       "       -0.061279 ,  0.71936  ,  0.32901  , -0.052772 ,  0.67135  ,\n",
       "       -0.80251  , -0.25789  ,  0.49615  ,  0.48081  , -0.68403  ,\n",
       "       -0.012239 ,  0.048201 ,  0.29461  ,  0.20614  ,  0.33556  ,\n",
       "       -0.64167  , -0.64708  ,  0.13377  , -0.12574  , -0.46382  ,\n",
       "        1.3878   ,  0.95636  , -0.067869 , -0.0017411,  0.52965  ,\n",
       "        0.45668  ,  0.61041  , -0.11514  ,  0.42627  ,  0.17342  ,\n",
       "       -0.7995   , -0.24502  , -0.60886  , -0.38469  , -0.4797   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['python']  # conversion of word python into a vector what the model sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors for words can be incroporated into a dataset as a features and the model can then use these vectors to classify the sentences as similar or dissimialr or positive negative etc. based on the model question. \n",
    "\n",
    "For Example, One can use the distances found above to find between positive key word and tell which words are closer to positive and which are closer to negative and based on that using a threshold classify words or sentences or texts as related to particular word or vector."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
